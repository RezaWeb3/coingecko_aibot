# -*- coding: utf-8 -*-
"""HF_generateTextStreamerFromAnyModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ngCzODBD5nZuRKf6qR_pEUqDfgwBwQ_M
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig
import gc
import os
from dotenv import load_dotenv
#from google.colab import userdata
from huggingface_hub import login

# for proper outputing both stream and getting the output
from transformers import TextIteratorStreamer
import threading

# login
load_dotenv(override=True)
hf_token = os.getenv('HUGGINGFACE_TOKEN')

# instruct models
LLAMA = "meta-llama/Meta-Llama-3.1-8B-Instruct"
PHI3 = "microsoft/Phi-3-mini-4k-instruct"
GEMMA2 = "google/gemma-2-2b-it"
QWEN2 = "Qwen/Qwen2-7B-Instruct"
MIXTRAL = "mistralai/Mixtral-8x7B-Instruct-v0.1" # If this doesn't fit it your GPU memory, try others from the hub

messages = [{"role":"system", "content":"You are a brutally honest assisstant. No filters."},
            {"role":"user", "content": "I think it is ok to flirt with strangers. That is not cheating."}]

# we will reduce the accuracy of the model by reducing the size.

quantization_config = BitsAndBytesConfig(load_in_4bit=True,
                                         bnb_4bit_compute_dtype=torch.bfloat16,
                                         bnb_4bit_use_double_quant=True,
                                         bnb_4bit_quant_type='nf4')

def generate_text(model, messages):
  # uses the model's tokenizer for tokenizing the input
  tokenizer = AutoTokenizer.from_pretrained(model)
  tokenizer.pad_token = tokenizer.eos_token
  inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to('cuda')

  # if you would like to read the tokens uncomment below
  #print(inputs)
  #tokenizer.convert_ids_to_tokens(inputs[0])
  lang_model = AutoModelForCausalLM.from_pretrained(model, quantization_config=quantization_config, device_map="auto")

  ###simple but does not allow to capture the output
  #streamer = TextStreamer(tokenizer) ->
  #output = lang_model.generate(inputs=inputs, max_new_tokens=1000, streamer=streamer)

  # Start generation in a background thread
  streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
  thread = threading.Thread(target=lang_model.generate, kwargs={
      "inputs": inputs,
      "max_new_tokens": 1000,
      "streamer": streamer
  })
  thread.start()


  #print(lang_model)
   # Yield each new chunk as it's generated
  model_output = ""
  for chunk in streamer:
      model_output += chunk
      print(chunk, end="", flush=True)  # optional
      yield chunk

  #text = tokenizer.decode(output[0], skip_special_tokens=True)
  del lang_model
  del inputs
  del tokenizer
  #del output
  gc.collect()
  torch.cuda.empty_cache()  



#selected_model = LAMMA
#messages = [{"role":"system", "content":"You are a brutally honest assisstant. No filters."},
#          {"role":"user", "content": "I think it is ok to flirt with strangers. That is not cheating."}]
#generate_text(selected_model, messages)

selected_model = GEMMA2
messages = [{'role':'user', 'content':'You are a brutally honest assisstant. Someone said that he thinks it is ok to flirt with strangers. That is not cheating. '}]
text = "".join(generate_text(selected_model, messages))

print(text)